.TH "doc_dev_data-structures_md" 3elektra "Sun Jul 10 2022" "Version 0.9.10" "Elektra" \" -*- nroff -*-
.ad l
.nh
.SH NAME
doc_dev_data-structures_md \- Data Structures 
 For an introduction, please \fBread first about elektra classes\fP\&. You might want to read \fBabout architecture first\fP\&.
.SH "Introduction"
.PP
Data structures define the common layer in Elektra\&. They are used for transferring configuration between Elektra and applications, but also between plugins\&.
.SS "ADT"
Both the \fCKeySet\fP and the interface to metadata within a \fCKey\fP are actually \fBADT\fP (abstract data types)\&. The API is designed so that different implementations of the data structures can be used internally\&.
.PP
A \fBhash\fP data structure presents a good candidate as alternative data structure, especially for the metadata interface\&. It is believed to be much faster on lookup, but considerably slower on sorted enumeration\&.
.PP
An \fBAVL tree\fP also serves as a competitor\&. AVL trees are expected to be faster for inserting keys at any place, but may be slower for appending because of the needed reorganizations\&. Their disadvantage is that they need to allocate a large number of small pieces of memory\&. Further investigations, namely implementation and benchmarks, are required to decide\&.
.PP
A \fBtrie\fP could also be used for lookup of key names\&. It performs well for lookup, but needs more memory allocations\&.
.PP
Currently the \fCKeySet\fP is implemented as a sorted array\&. It is fast on appending and iterating, and has nearly no size-overhead\&. To improve the lookup-time, an additional \fBhash\fP will be used\&.
.SS "ABI compatibility"
Application binary interface, or ABI, is the interface to all data structures of an application or library directly allocated or accessed by the user\&.
.PP
Special care has been taken in Elektra to support all changes within the data structures without any ABI changes\&. ABI changes would entail the recompilation of applications and plugins using Elektra\&. The functions \fC\fBkeyNew()\fP\fP, \fC\fBksNew()\fP\fP and \fC\fBkdbOpen()\fP\fP allocate the data structures for the applications\&. The user only gets pointers to them\&. It is not possible for the user to allocate or access these data structures directly when only using the public header file \fC<kdb\&.h>\fP\&. The functions \fC\fBkeyDel()\fP\fP, \fC\fBksDel()\fP\fP and \fC\fBkdbClose()\fP\fP free the resources after use\&. Using the C++ binding deallocation is done automatically\&.
.SH "Metadata"
.PP
Read \fBhere\fP\&.
.SH "KeySet"
.PP
This subsection describes what has changed between 0\&.7 and 0\&.8 and deals with some general implementation issues\&.
.SS "Operations"
\fCKeySet\fP resembles the classical mathematical set\&. Operations like union, intersection or difference are well-defined\&. In mathematics typically every operation yields a new set\&. Instead, we try to reuse sets in the following ways:
.PP
.IP "\(bu" 2
A completely new and independent \fCKeySet\fP as return value would resemble the mathematical ideal closely\&. This operation would be expensive\&. Every \fCKey\fP needs to be duplicated and inserted into a new \fCKeySet\fP\&.
.PP
Such a \fBdeep duplication\fP was only needed in \fC\fBkdbSet()\fP\fP\&.
.IP "\(bu" 2
The resulting \fCKeySet\fP is created during the operation, but only a flat copy is made\&. This means that the keys in it are actually not duplicated, but only their reference counter is increased\&. This method is similar to the mathematical model\&. Compared with a deep copy it can achieve good performance\&. But all changes to the values of keys in the resulting \fCKeySet\fP affect the original \fCKeySet\fP, too\&.
.PP
\fC\fBksDup(const KeySet *source)\fP\fP produces a new \fCKeySet\fP\&. That way the \fCsource\fP is not changed as shown by the \fCconst\fP modifier\&.
.IP "\(bu" 2
The result of the operation is applied to the \fCKeySet\fP passed as argument directly\&. This is actually quite common, but for this situation other names of the operations are more suitable\&.
.PP
For example, a union which changes the \fCKeySet\fP is called \fC\fBksAppend()\fP\fP\&.
.IP "\(bu" 2
A new \fCKeySet\fP is created, but the \fCKeySet\fP passed as parameter is reduced by the keys needed for the new \fCKeySet\fP\&. This is useful in situations where many operations have to be applied in a sequence reducing the given \fCKeySet\fP until no more keys are left\&. None of the reference pointers change in this situation\&.
.PP
\fC\fBksCut(KeySet *ks, const Key *cutpoint)\fP\fP works that way\&. All keys below the \fCcutpoint\fP are moved from \fCks\fP to the returned key set\&.
.PP
.SS "Consistency"
There are several ways to define consistency relations on key sets\&. For \fBstrict consistency\fP every parent key must exist before the user can append a key to a key set\&. For example, the key set with the keys
.PP
.PP
.nf
system:/
system:/elektra
system:/elektra/mountpoints
.fi
.PP
.PP
would allow the key \fCsystem:/elektra/mountpoints/tcl\fP to be added, but not the key \fCsystem:/apps/abc\fP because \fCsystem:/apps\fP is missing\&. File systems enforce this kind of consistency\&.
.PP
These semantics are however not useful for configurations\&. Especially for user configurations often only some keys need to be overwritten\&. It is not a good idea to copy all parent keys to the users' configuration\&. For this reason we use a less strict definition of consistency supporting such holes\&.
.PP
We also evaluated a form of \fBweak consistency\fP\&. It avoids adding some unnecessary keys\&. A constraint is that a key can be added only if it has a parent key\&. But the constraint does not apply if no other key exists above the key about to be inserted\&. From that moment it will serve as parent key for other keys\&. With the current implementation of \fCKeySet\fP, however, it is not possible to decide this constraint in constant time\&. Instead its worst-case complexity would be $log(n) * x$ where $n$ is the number of keys currently in the key set and $x$ is the \fIdepth\fP of the key\&. The depth is the number of \fC/\fP in the key name\&. The worst-case of the complexity applies when the inserting works without a parent key\&. For example, with the keys
.PP
.PP
.nf
user:/sw/apps/abc/current/bindings
user:/sw/apps/abc/current/bindings/key1
user:/sw/apps/abc/current/bindings/key2
.fi
.PP
.PP
the weak consistency would allow inserting \fCuser:/sw/apps/abc/current/bindings/key3\fP because it is directly below an existing key\&. It would also allow adding \fCuser:/sw/apps/xyz/current\fP because it does not have any parent key\&. But it would not allow \fCuser:/sw/apps/abc/current/bindings/dir/key1\fP to add\&. The worst-case complexity was found to be too expensive, and hence \fCKeySet\fP has \fBno consistency\fP check at all\&.
.PP
This means any key with a valid key name can be inserted into \fCKeySet\fP\&. The \fCKeySet\fP is changed so that it is now impossible to append keys without a name\&. \fCksAppendKey(ks, Key *toAppend)\fP takes ownership of the key \fCtoAppend\fP and will delete the key in that case\&. The caller does not have to free \fCtoAppend\fP: either it is in the key set or it is deleted\&.
.PP
Binary search determines the position where to insert a key\&. The C version of binary search \fCbsearch()\fP cannot tell where to insert a key when it is not found\&. So the algorithm has to be reimplemented\&. Java's binary search \fCbinarySearch()\fP uses a trick to both indicate where a key is found and where to insert it with the same return code by returning the negative value \fC((-insertion point) - 1)\fP indicating where the new value should be inserted when the key is not found\&. Elektra now also uses this trick internally\&.
.SS "Iteration"
Iterating over a \fCKeySet\fP is similar to iterating over arrays\&. With \fCssize_t ksGetSize (const KeySet *ks)\fP, the total number of \fCKey\fPs in a \fCKeySet\fP can be retrieved and with the function \fCKey *ksAtCursor (const KeySet *ks, elektraCursor cursor)\fP the \fCKey *\fP at the specified position \fCcursor\fP in \fCks\fP can be retrieved\&. The first element (\fCKey\fP) has the index 0, so the last \fCKey\fP in the \fCKeySet\fP \fCks\fP can be accessed with \fCKey * k = ksAtCursor (ks, ksGetSize (ks) - 1)\fP\&. Please be aware the elements in a \fCKeySet\fP can move and therefore change their index, e\&.g\&. when deleting or adding elements or using \fCksCut ()\fP\&.
.PP
.PP
.nf
for (elektraCursor it = 0; it < ksGetSize (ks); ++it)
{
        Key * cur = ksAtCursor (ks, it);
        // \&.\&.\&.
}
.fi
.PP
.SH "Trie vs\&. Split"
.PP
Up to now, we have discussed external data structures visible to the user of the library\&. The application and plugin programmer needs them to access configuration\&. Last, but not least, we will show two internal data structures\&. The user will not see them\&. To understand the algorithm, however, the user needs to understand them as well\&.
.SS "Trie"
A \fITrie\fP or prefix tree is an ordered tree data structure\&. In Elektra, it provides the information to decide in which backend a key resides\&. The algorithm, presented in \fBalgorithm\fP, also needs a list of all backends\&. The initial approach was to iterate over the \fCTrie\fP to get a list of all backends\&. But the transformation of a \fCTrie\fP to a list of backends, contained many bugs caused by corner cases in connection with the default backend and cascading mount points\&.
.SS "Split"
So, instead of transforming the trie to a list of backends, we introduced a new data structure called \fCSplit\fP\&. The name \fCSplit\fP comes from the fact that an initial key set is split into many key sets\&. These key sets are stored in the \fCSplit\fP object\&. \fCSplit\fP advanced to the central data structure for the algorithm:
.PP
.PP
.nf
typedef struct _Split   Split;

struct _Split {
        size_t size;
        size_t alloc;
        KeySet **keysets;
        Backend **handles;
        Key **parents;
        int *syncbits;
};
.fi
.PP
.PP
The data structure \fCSplit\fP contains the following fields:
.PP
.IP "\(bu" 2
\fBsize\fP: contains the number of key sets currently in \fCSplit\fP\&.
.IP "\(bu" 2
\fBalloc\fP: allows us to allocate more items than currently in use\&.
.IP "\(bu" 2
\fBkeysets\fP represents a list of key sets\&. The keys in one of the key sets are known to belong to a specific backend\&.
.IP "\(bu" 2
\fBhandles\fP: contains a list of handles to backends\&.
.IP "\(bu" 2
\fBparents\fP: represents a list of keys\&. Each \fCparentKey\fP contains the root key of a backend\&. No key of the respective key set is above the \fCparentKey\fP\&. The key name of \fCparentKey\fP contains the mount point of a backend\&. The resolver writes the file name into the value of the \fCparentKey\fP\&.
.IP "\(bu" 2
\fBsyncbits\fP: are some bits that can be set for every backend\&. The algorithm uses the \fCsyncbits\fP to decide if the key set needs to be synchronized\&.
.PP
.PP
Continue reading \fBwith the error handling\fP\&.
.SH "Order Preserving Minimal Perfect Hash Map (aka OPMPHM)"
.PP
The OPMPHM is a non-dynamic randomized hash map of the Las Vegas type, that creates an index over the elements, to gain O(1) access\&.
.PP
The elements must be arranged in an array and each element must have a unique name, to identify the elements\&. The source can be found in \fCkdbopmphm\&.h\fP and \fCopmphm\&.c\fP and also works outside of Elektra\&.
.PP
The OPMPHM does not store any buckets, your array of elements are the buckets and the OPMPHM represent an arbitrary index over those\&. The desired index of an element, also known as the order, is set in \fCOpmphmGraph->edges[i]\&.order\fP, where \fCi\fP is the i-th element in your array\&. When the orders should represent the array indices, the default order can be applied during the assignment step\&. When the orders are not the default order, \fCOpmphmGraph->edges[i]\&.order\fP should be set before the assignment step\&.
.PP
Because the OPMPHM is non-dynamic, there are no insert and delete operations\&. The OPMPHM gets build for a static set of elements, once the OPMPHM is build, every:
.PP
.IP "\(bu" 2
change of at least one indexed element name
.IP "\(bu" 2
addition of a new element
.IP "\(bu" 2
deletion of an indexed element
.PP
.PP
leads to an invalid OPMPHM and forces a rebuild\&. A build consists of two steps the mapping step and the assignment step\&.
.PP
During the mapping step the OPMPHM maps each element to an edge in a random acyclic r-uniform r-partite hypergraph\&. In a r-uniform r-partite hypergraph each edge connects \fCr\fP vertices, each vertex in a different component\&. The probability of being acyclic and the number of mapping step invocations depends on the following variables:
.PP
.IP "\(bu" 2
\fCr\fP: The \fCr\fP variable defines the number of components in the random r-uniform r-partite hypergraph\&. Use the \fCopmphmOptR (n)\fP function to get an optimal value for your number of elements (\fCn\fP)\&.
.IP "\(bu" 2
\fCc\fP: The \fCc\fP variable defines the number of vertices in each component of the random r-uniform r-partite hypergraph\&. The number of vertices in one component is defined as \fC(c * n / r) + 1\fP, where \fCn\fP is the number of elements and \fCr\fP is the variable from above\&. The \fCc\fP variable must have a minimal value to ensure a success probability, use the \fCopmphmMinC (r)\fP function, with your \fCr\fP from above\&. To ensure an optimal time until success increment the \fCc\fP variable with the value from the \fCopmphmOptC (n)\fP function, where \fCn\fP is the number of elements\&.
.IP "\(bu" 2
\fCinitSeed\fP: The initial seed set in \fCOpmphmInit->initSeed\fP\&.
.PP
.PP
\fCopmphmOptR (n)\fP and \fCopmphmOptC (n)\fP are heuristic functions constructed through benchmarks\&. Optimal is only one mapping step invocation in 99\&.5% of the observed cases\&. The benchmarks took arbitrary uniform distributed initial seeds and the heuristic functions are made to work with almost every seed\&.
.SS "The Build"
.SS "Initialization"
Use \fCopmphmNew ()\fP and \fCopmphmGraphNew (\&.\&.\&.)\fP to instantiate the needed structures\&. The function \fCopmphmGraphNew (\&.\&.\&.)\fP takes \fCr\fP and \fCc\fP as parameter\&. Use the \fCopmphmOptR (\&.\&.\&.)\fP function to get your \fCr\fP value, use this \fCr\fP also to get your \fCc\fP value the following way:
.PP
\fCc = opmphmMinC (r) + opmphmOptC (n)\fP
.PP
To initialize the OPMPHM build the \fCOpmphmInit\fP must be set with information about your data\&. Set your data array \fCOpmphmInit->data\fP and the element name extraction function \fCOpmphmInit->getName\fP, which should extract the string from a single data array entry\&. Provide a good seed in \fCOpmphmInit->initSeed\fP, needed in the next step\&.
.SS "Mapping"
The function \fCopmphmMapping\fP uses your seed (the \fCOpmphmInit->seed\fP will be changed) and tries to construct the random acyclic r-uniform r-partite hypergraph, this might not succeed, on cycles just call it again\&.
.SS "Assignment"
The \fCopmphmAssignment ()\fP function assigns either your order (set at \fCOpmphmGraph->edges[i]\&.order\fP) or a default order\&. The \fCdefaultOrder\fP parameter indicates the behavior\&.
.PP
After the build the OpmphmInit and OpmphmGraph should be freed\&. The OPMPHM is now ready for constant lookups with the \fCopmphmLookup ()\fP\&.
.SS "The Rebuild"
Once build, follow the steps from the build, just omit the \fCopmphmNew ()\fP invocation\&. 
